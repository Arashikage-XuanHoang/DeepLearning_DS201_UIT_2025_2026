{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Bài 3: Xây dựng kiến trúc Encoder-Decoder gồm 3 lớp LSTM cho module encoder và 3 lớp LSTM cho module decoder, với hidden size là 256, cho bài toán dịch máy từ tiếng Anh sang tiếng Việt. Module decoder được trang bị kỹ thuật attention theo mô tả của nghiên cứu \"[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\". Huấn luyện mô hình này trên bộ dữ liệu PhoMT sử dụng Adam làm phương thức tối ưu tham số. Đánh giá độ hiệu quả của mô hình sử dụn độ đo ROUGE-L."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PdC1E_j9kx_",
        "outputId": "77036aca-1d3d-4247-bdc2-25df9ca16d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file 1hoTd2hFwjSeFThlPm6YpN0NW5ePXS3Jc small-dev.json\n",
            "Processing file 1_3L25SH1_jaEfOjpmpgnfMik4N3MxSyn small-test.json\n",
            "Processing file 1-eG6FeF-v__rsf77iWurddahXbyjTYh5 small-train.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hoTd2hFwjSeFThlPm6YpN0NW5ePXS3Jc\n",
            "To: /content/small-PhoMT/small-dev.json\n",
            "100%|██████████| 594k/594k [00:00<00:00, 118MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_3L25SH1_jaEfOjpmpgnfMik4N3MxSyn\n",
            "To: /content/small-PhoMT/small-test.json\n",
            "100%|██████████| 669k/669k [00:00<00:00, 111MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-eG6FeF-v__rsf77iWurddahXbyjTYh5\n",
            "To: /content/small-PhoMT/small-train.json\n",
            "100%|██████████| 5.68M/5.68M [00:00<00:00, 43.2MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['/content/small-PhoMT/small-dev.json',\n",
              " '/content/small-PhoMT/small-test.json',\n",
              " '/content/small-PhoMT/small-train.json']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install gdown --upgrade\n",
        "\n",
        "import gdown\n",
        "\n",
        "folder_id = \"186OAOuSEYEDVcry7WP5UBdqECXo26QAb\"\n",
        "gdown.download_folder(id=folder_id, quiet=False, use_cookies=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0Gkj_gI9wnj",
        "outputId": "00a0b89b-9052-4752-ea2a-0b7135fd34a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "WugrK4a79y4v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import json\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CVwRHZPb96Rg"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.pad_idx = 0\n",
        "        self.bos_idx = 1\n",
        "        self.eos_idx = 2\n",
        "        self.src_word2idx = {'<pad>': 0, '<bos>': 1, '<eos>': 2}\n",
        "        self.tgt_word2idx = {'<pad>': 0, '<bos>': 1, '<eos>': 2}\n",
        "        self.src_idx2word = {0: '<pad>', 1: '<bos>', 2: '<eos>'}\n",
        "        self.tgt_idx2word = {0: '<pad>', 1: '<bos>', 2: '<eos>'}\n",
        "\n",
        "    def build(self, pairs):\n",
        "        src_id = 3\n",
        "        tgt_id = 3\n",
        "        for en, vi in pairs:\n",
        "            for w in en.split():\n",
        "                if w not in self.src_word2idx:\n",
        "                    self.src_word2idx[w] = src_id\n",
        "                    self.src_idx2word[src_id] = w\n",
        "                    src_id += 1\n",
        "            for w in vi.split():\n",
        "                if w not in self.tgt_word2idx:\n",
        "                    self.tgt_word2idx[w] = tgt_id\n",
        "                    self.tgt_idx2word[tgt_id] = w\n",
        "                    tgt_id += 1\n",
        "        self.total_src_tokens = src_id\n",
        "        self.total_tgt_tokens = tgt_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JUVYhBb8-Hq_"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, vocab):\n",
        "        self.pairs = pairs\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en, vi = self.pairs[idx]\n",
        "        en_ids = [self.vocab.bos_idx] + [self.vocab.src_word2idx.get(w, 0) for w in en.split()] + [self.vocab.eos_idx]\n",
        "        vi_ids = [self.vocab.bos_idx] + [self.vocab.tgt_word2idx.get(w, 0) for w in vi.split()] + [self.vocab.eos_idx]\n",
        "        return torch.tensor(en_ids), torch.tensor(vi_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Wrb81QAV-LCR"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    src, tgt = zip(*batch)\n",
        "    src = nn.utils.rnn.pad_sequence(src, padding_value=0, batch_first=True)\n",
        "    tgt = nn.utils.rnn.pad_sequence(tgt, padding_value=0, batch_first=True)\n",
        "    return src, tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "piqFvSuQ-Oe_"
      },
      "outputs": [],
      "source": [
        "class Seq2seqLSTM(nn.Module):\n",
        "    def __init__(self, d_model: int, n_layers: int, dropout: float, vocab: Vocab):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.n_layers = n_layers\n",
        "        self.d_model = d_model\n",
        "        self.dim = 2 * d_model\n",
        "        self.src_embedding = nn.Embedding(vocab.total_src_tokens, d_model, vocab.pad_idx)\n",
        "        self.encoder = nn.LSTM(d_model, d_model, n_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.tgt_embedding = nn.Embedding(vocab.total_tgt_tokens, self.dim, vocab.pad_idx)\n",
        "        self.decoder = nn.LSTM(self.dim, self.dim, n_layers, batch_first=True, dropout=dropout, bidirectional=False)\n",
        "        self.attn_W = nn.Linear(self.dim, self.dim)\n",
        "        self.output_head = nn.Linear(self.dim * 2, vocab.total_tgt_tokens)\n",
        "        self.loss = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)\n",
        "\n",
        "    def get_encoder_states(self, embedded_x):\n",
        "        enc_outputs, (enc_h, enc_c) = self.encoder(embedded_x)\n",
        "        bs = enc_outputs.shape[0]\n",
        "        enc_h = enc_h.reshape(self.n_layers, 2, bs, self.d_model).permute(0, 2, 1, 3).reshape(self.n_layers, bs, self.dim)\n",
        "        enc_c = enc_c.reshape(self.n_layers, 2, bs, self.d_model).permute(0, 2, 1, 3).reshape(self.n_layers, bs, self.dim)\n",
        "        return enc_outputs, enc_h, enc_c\n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "        self.train()\n",
        "        embedded_x = self.src_embedding(x)\n",
        "        enc_outputs, enc_h, enc_c = self.get_encoder_states(embedded_x)\n",
        "        bs, tgt_len = y.shape\n",
        "        dec_hidden = enc_h\n",
        "        dec_cell = enc_c\n",
        "        logits = []\n",
        "        for ith in range(tgt_len):\n",
        "            y_ith = y[:, ith].unsqueeze(1)\n",
        "            embedded_y = self.tgt_embedding(y_ith)\n",
        "            dec_output, (dec_hidden, dec_cell) = self.decoder(embedded_y, (dec_hidden, dec_cell))\n",
        "            query = dec_output.squeeze(1)\n",
        "            context = self.aligning(query, enc_outputs)\n",
        "            combined = torch.cat([query, context], dim=-1)\n",
        "            logit = self.output_head(combined)\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "        logits = torch.cat(logits, dim=1)\n",
        "        loss = self.loss(logits.reshape(-1, self.vocab.total_tgt_tokens), y.reshape(-1))\n",
        "        return loss\n",
        "\n",
        "    def aligning(self, query: torch.Tensor, k_v: torch.Tensor):\n",
        "        query = query.unsqueeze(1)\n",
        "        keys = self.attn_W(k_v)\n",
        "        scores = torch.bmm(query, keys.transpose(1, 2))\n",
        "        a = nn.functional.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(a, k_v).squeeze(1)\n",
        "        return context\n",
        "\n",
        "    def predict(self, x: torch.Tensor, max_len=100):\n",
        "        self.eval()\n",
        "        embedded_x = self.src_embedding(x)\n",
        "        enc_outputs, enc_h, enc_c = self.get_encoder_states(embedded_x)\n",
        "        bs = x.shape[0]\n",
        "        dec_hidden = enc_h\n",
        "        dec_cell = enc_c\n",
        "        y_ith = torch.full((bs, 1), self.vocab.bos_idx, dtype=torch.long, device=x.device)\n",
        "        outputs = []\n",
        "        for _ in range(max_len):\n",
        "            embedded_y = self.tgt_embedding(y_ith)\n",
        "            dec_output, (dec_hidden, dec_cell) = self.decoder(embedded_y, (dec_hidden, dec_cell))\n",
        "            query = dec_output.squeeze(1)\n",
        "            context = self.aligning(query, enc_outputs)\n",
        "            combined = torch.cat([query, context], dim=-1)\n",
        "            logit = self.output_head(combined)\n",
        "            y_ith = logit.argmax(dim=-1).unsqueeze(1)\n",
        "            outputs.append(y_ith)\n",
        "            if all(y_ith.squeeze(1) == self.vocab.eos_idx):\n",
        "                break\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OXQZeuV-TMp",
        "outputId": "3a67c63d-6b0e-489f-ac17-b0054980fd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 20000 câu\n",
            "Dev: 2000 câu\n",
            "Test: 2000 câu\n"
          ]
        }
      ],
      "source": [
        "# Load data from JSON files\n",
        "def load_data(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    pairs = [(item[\"english\"], item[\"vietnamese\"]) for item in data]\n",
        "    return pairs\n",
        "\n",
        "train_pairs = load_data(\"/content/small-PhoMT/small-train.json\")\n",
        "dev_pairs = load_data(\"/content/small-PhoMT/small-dev.json\")\n",
        "test_pairs = load_data(\"/content/small-PhoMT/small-test.json\")\n",
        "\n",
        "print(f\"Train: {len(train_pairs)} câu\")\n",
        "print(f\"Dev: {len(dev_pairs)} câu\")\n",
        "print(f\"Test: {len(test_pairs)} câu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "GWpDOBYq-Vyf"
      },
      "outputs": [],
      "source": [
        "# Xây dựng vocab\n",
        "vocab = Vocab()\n",
        "vocab.build(train_pairs)\n",
        "\n",
        "# Datasets và dataloaders\n",
        "train_dataset = TranslationDataset(train_pairs, vocab)\n",
        "dev_dataset = TranslationDataset(dev_pairs, vocab)\n",
        "test_dataset = TranslationDataset(test_pairs, vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLtomwTN9U3f",
        "outputId": "854f2033-1cef-4850-a114-d73245860d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 5.6776\n",
            "Epoch 2, Loss: 3.5587\n",
            "Epoch 3, Loss: 2.9841\n",
            "Epoch 4, Loss: 2.7201\n",
            "Epoch 5, Loss: 2.5173\n",
            "Epoch 6, Loss: 2.3008\n",
            "Epoch 7, Loss: 2.1018\n",
            "Epoch 8, Loss: 1.9447\n",
            "Epoch 9, Loss: 1.7695\n",
            "Epoch 10, Loss: 1.5207\n"
          ]
        }
      ],
      "source": [
        "# Khởi tạo mô hình\n",
        "d_model = 128  # Để hidden size decoder = 256\n",
        "n_layers = 5\n",
        "dropout = 0.3\n",
        "model = Seq2seqLSTM(d_model, n_layers, dropout, vocab)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Huấn luyện\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        loss = model(x, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lbgyU_vMUqj",
        "outputId": "6410234b-22bd-4f22-9d7a-db395607a752"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O24oB9kL-jd",
        "outputId": "4c2b72df-3a36-475f-b6dd-78989ca4fcf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU@1: 0.0002\n",
            "BLEU@2: 0.0001\n",
            "BLEU@3: 0.0000\n",
            "BLEU@4: 0.0000\n",
            "ROUGE-1: 0.0471\n",
            "ROUGE-2: 0.0047\n",
            "ROUGE-L: 0.0389\n",
            "Meteor: 0.0006\n"
          ]
        }
      ],
      "source": [
        "# Đánh giá\n",
        "def decode(ids, idx2word):\n",
        "    sentence = []\n",
        "    for id in ids:\n",
        "        if id == 0: continue\n",
        "        if id == 2: break\n",
        "        sentence.append(idx2word.get(id, '<unk>'))\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "bleu1, bleu2, bleu3, bleu4 = [], [], [], []\n",
        "rouge1, rouge2, rougel = [], [], []\n",
        "meteors = []\n",
        "\n",
        "smoother = SmoothingFunction()\n",
        "r_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device)\n",
        "        preds = model.predict(x)\n",
        "        for pred, ref in zip(preds.tolist(), y.tolist()):\n",
        "            pred_str = decode(pred, vocab.tgt_idx2word)\n",
        "            ref_str = decode(ref[1:], vocab.tgt_idx2word)  # Bỏ <bos>\n",
        "            ref_list = ref_str.split()\n",
        "            pred_list = pred_str.split()\n",
        "            bleu1.append(sentence_bleu([ref_list], pred_list, weights=(1, 0, 0, 0), smoothing_function=smoother.method1))\n",
        "            bleu2.append(sentence_bleu([ref_list], pred_list, weights=(0.5, 0.5, 0, 0), smoothing_function=smoother.method1))\n",
        "            bleu3.append(sentence_bleu([ref_list], pred_list, weights=(1/3, 1/3, 1/3, 0), smoothing_function=smoother.method1))\n",
        "            bleu4.append(sentence_bleu([ref_list], pred_list, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoother.method1))\n",
        "            scores = r_scorer.score(ref_str, pred_str)\n",
        "            rouge1.append(scores['rouge1'].fmeasure)\n",
        "            rouge2.append(scores['rouge2'].fmeasure)\n",
        "            rougel.append(scores['rougeL'].fmeasure)\n",
        "            meteors.append(meteor_score([ref_list], pred_list))\n",
        "\n",
        "print(f\"BLEU@1: {np.mean(bleu1):.4f}\")\n",
        "print(f\"BLEU@2: {np.mean(bleu2):.4f}\")\n",
        "print(f\"BLEU@3: {np.mean(bleu3):.4f}\")\n",
        "print(f\"BLEU@4: {np.mean(bleu4):.4f}\")\n",
        "print(f\"ROUGE-1: {np.mean(rouge1):.4f}\")\n",
        "print(f\"ROUGE-2: {np.mean(rouge2):.4f}\")\n",
        "print(f\"ROUGE-L: {np.mean(rougel):.4f}\")\n",
        "print(f\"Meteor: {np.mean(meteors):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
