{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQykDNv4wZD_"
      },
      "source": [
        "# BÀI THỰC HÀNH 4: MẠNG NEURAL HỒI QUY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHMYhWojwZEB"
      },
      "source": [
        "<b>Hướng dẫn nộp bài:</b> Các bạn commit và push code lên github, sử dụng file txt đặt tên theo cú pháp <MSSV>.txt chứa đường link dẫn đến github của bài thực hành và nộp file txt này tên courses.\n",
        "\n",
        "Bộ dữ liệu sử dụng: [PhoMT](https://drive.google.com/drive/folders/1ksAAeUq2b4u_hiKpyzaEDNzfQDHoB1cI?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_9tYiu_wZEB"
      },
      "source": [
        "#### Bài 1: Xây dựng kiến trúc Encoder-Decoder gồm 5 lớp LSTM cho module encoder và 5 lớp LSTM cho module decoder, với hidden size là 256, cho bài toán dịch máy từ tiếng Anh sang tiếng Việt. Huấn luyện mô hình này trên bộ dữ liệu PhoMT sử dụng Adam làm phương thức tối ưu tham số. Đánh giá độ hiệu quả của mô hình sử dụng các độ đo BLEU (BLEU@1, BLEU@2, BLEU@3, BLEU@4), ROUGE, và Meteor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1vbdpQA4srG",
        "outputId": "e9ef51e4-e9f5-4ef6-d959-87f951177925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file 1hoTd2hFwjSeFThlPm6YpN0NW5ePXS3Jc small-dev.json\n",
            "Processing file 1_3L25SH1_jaEfOjpmpgnfMik4N3MxSyn small-test.json\n",
            "Processing file 1-eG6FeF-v__rsf77iWurddahXbyjTYh5 small-train.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hoTd2hFwjSeFThlPm6YpN0NW5ePXS3Jc\n",
            "To: /content/small-PhoMT/small-dev.json\n",
            "100%|██████████| 594k/594k [00:00<00:00, 123MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_3L25SH1_jaEfOjpmpgnfMik4N3MxSyn\n",
            "To: /content/small-PhoMT/small-test.json\n",
            "100%|██████████| 669k/669k [00:00<00:00, 121MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-eG6FeF-v__rsf77iWurddahXbyjTYh5\n",
            "To: /content/small-PhoMT/small-train.json\n",
            "100%|██████████| 5.68M/5.68M [00:00<00:00, 309MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['/content/small-PhoMT/small-dev.json',\n",
              " '/content/small-PhoMT/small-test.json',\n",
              " '/content/small-PhoMT/small-train.json']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cài đặt gdown (nếu chưa có)\n",
        "!pip install gdown --upgrade\n",
        "\n",
        "import gdown\n",
        "\n",
        "folder_id = \"186OAOuSEYEDVcry7WP5UBdqECXo26QAb\"\n",
        "gdown.download_folder(id=folder_id, quiet=False, use_cookies=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lRIFEaA4uVd",
        "outputId": "a1242e3c-f663-459e-b881-5f0dd9a6d900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 20000 câu\n",
            "Dev: 2000 câu\n",
            "Test: 2000 câu\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def load_data(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    # Giả sử định dạng là list các dict với key \"en\" và \"vi\"\n",
        "    pairs = [(item[\"english\"], item[\"vietnamese\"]) for item in data]\n",
        "    return pairs\n",
        "\n",
        "train_pairs = load_data(\"/content/small-PhoMT/small-train.json\")\n",
        "dev_pairs = load_data(\"/content/small-PhoMT/small-dev.json\")\n",
        "test_pairs = load_data(\"/content/small-PhoMT/small-test.json\")\n",
        "\n",
        "print(f\"Train: {len(train_pairs)} câu\")\n",
        "print(f\"Dev: {len(dev_pairs)} câu\")\n",
        "print(f\"Test: {len(test_pairs)} câu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq8P5c0I5HaC",
        "outputId": "7a2314aa-0a7c-4725-942a-1b28f684d2f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab EN mới: 6000 từ\n",
            "Vocab VI mới: 5000 từ\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Chuẩn hóa unicode và chuyển về lowercase (chỉ với tiếng Anh)\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    text = text.lower()\n",
        "    # Loại bỏ ký tự đặc biệt, giữ lại chữ cái, số và khoảng trắng\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Tách từ đơn giản (word level)\n",
        "def tokenize_en(text):\n",
        "    return normalize_text(text).split()\n",
        "\n",
        "def tokenize_vi(text):\n",
        "    # Tiếng Việt giữ nguyên dấu, chỉ chuẩn hóa unicode và loại bỏ ký tự lạ\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠ-ỹ\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text.split()\n",
        "\n",
        "# Xây dựng từ vựng\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "        self.add_word('<PAD>')\n",
        "        self.add_word('<SOS>')\n",
        "        self.add_word('<EOS>')\n",
        "        self.add_word('<UNK>')\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def add_sentence(self, sentence, tokenizer):\n",
        "        for word in tokenizer(sentence):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.idx\n",
        "\n",
        "# Xây dựng vocab cho source (en) và target (vi)\n",
        "def build_vocabs(train_pairs, max_vocab_size=8000):\n",
        "    src_vocab = Vocabulary()\n",
        "    tgt_vocab = Vocabulary()\n",
        "\n",
        "    src_counter = Counter()\n",
        "    tgt_counter = Counter()\n",
        "\n",
        "    for en, vi in train_pairs:\n",
        "        src_counter.update(tokenize_en(en))\n",
        "        tgt_counter.update(tokenize_vi(vi))\n",
        "\n",
        "    # Chỉ lấy các từ xuất hiện ít nhất 2 lần + top frequent\n",
        "    for word, freq in src_counter.items():\n",
        "        if freq >= 2:\n",
        "            src_vocab.add_word(word)\n",
        "    for word, freq in tgt_counter.items():\n",
        "        if freq >= 2:\n",
        "            tgt_vocab.add_word(word)\n",
        "\n",
        "    # Giới hạn cứng vocab size\n",
        "    if len(src_vocab) > max_vocab_size:\n",
        "        # giữ lại top (max_vocab_size - 4) từ phổ biến nhất\n",
        "        top_words = [w for w, _ in src_counter.most_common(max_vocab_size - 4)]\n",
        "        src_vocab = Vocabulary()\n",
        "        for w in top_words:\n",
        "            src_vocab.add_word(w)\n",
        "\n",
        "    if len(tgt_vocab) > max_vocab_size:\n",
        "        top_words = [w for w, _ in tgt_counter.most_common(max_vocab_size - 4)]\n",
        "        tgt_vocab = Vocabulary()\n",
        "        for w in top_words:\n",
        "            tgt_vocab.add_word(w)\n",
        "\n",
        "    return src_vocab, tgt_vocab\n",
        "\n",
        "src_vocab, tgt_vocab = build_vocabs(train_pairs, max_vocab_size=6000)\n",
        "\n",
        "print(f\"Vocab EN mới: {len(src_vocab)} từ\")\n",
        "print(f\"Vocab VI mới: {len(tgt_vocab)} từ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy-itYr55JPx",
        "outputId": "8a21f9e9-1b97-406f-ff1d-16dafc31b05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataLoader đã sẵn sàng!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, src_vocab, tgt_vocab):\n",
        "        self.pairs = pairs\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en, vi = self.pairs[idx]\n",
        "\n",
        "        src = [self.src_vocab.word2idx.get(w, self.src_vocab.word2idx['<UNK>']) for w in tokenize_en(en)]\n",
        "        src = [self.src_vocab.word2idx['<SOS>']] + src + [self.src_vocab.word2idx['<EOS>']]\n",
        "\n",
        "        tgt = [self.tgt_vocab.word2idx.get(w, self.tgt_vocab.word2idx['<UNK>']) for w in tokenize_vi(vi)]\n",
        "        tgt = [self.tgt_vocab.word2idx['<SOS>']] + tgt + [self.tgt_vocab.word2idx['<EOS>']]\n",
        "\n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    srcs, tgts = zip(*batch)\n",
        "    srcs_padded = pad_sequence(srcs, batch_first=True, padding_value=src_vocab.word2idx['<PAD>'])\n",
        "    tgts_padded = pad_sequence(tgts, batch_first=True, padding_value=tgt_vocab.word2idx['<PAD>'])\n",
        "    return srcs_padded, tgts_padded\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = TranslationDataset(train_pairs, src_vocab, tgt_vocab)\n",
        "dev_dataset   = TranslationDataset(dev_pairs, src_vocab, tgt_vocab)\n",
        "test_dataset  = TranslationDataset(test_pairs, src_vocab, tgt_vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader   = DataLoader(dev_dataset,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "print(\"DataLoader đã sẵn sàng!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4noWW695Ks2",
        "outputId": "b150198b-a837-460e-9c08-ac132a7fb97c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(6000, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, num_layers=5, batch_first=True, dropout=0.3)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(5000, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, num_layers=5, batch_first=True, dropout=0.3)\n",
            "    (fc): Linear(in_features=256, out_features=5000, bias=True)\n",
            "  )\n",
            ")\n",
            "Số tham số: 9,364,360\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=5, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=src_vocab.word2idx['<PAD>'])\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        _, (hidden, cell) = self.lstm(emb)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=5, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=tgt_vocab.word2idx['<PAD>'])\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        emb = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(emb, (hidden, cell))\n",
        "        pred = self.fc(output)\n",
        "        return pred, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(src.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        input_token = tgt[:, 0].unsqueeze(1)  # <SOS>\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
        "            outputs[:, t, :] = output.squeeze(1)\n",
        "\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(-1)\n",
        "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# Khởi tạo mô hình\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "num_layers = 5\n",
        "\n",
        "encoder = Encoder(len(src_vocab), embed_size, hidden_size, num_layers)\n",
        "decoder = Decoder(len(tgt_vocab), embed_size, hidden_size, num_layers)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "\n",
        "\n",
        "print(model)\n",
        "print(f\"Số tham số: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRdoFaEu5MHc",
        "outputId": "67284b9e-38b5-4481-8fd8-cbf0a84d72fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 | Loss: 6.4107\n",
            "Epoch  2 | Loss: 6.2936\n",
            "Epoch  3 | Loss: 6.2811\n",
            "Epoch  4 | Loss: 6.2665\n",
            "Epoch  5 | Loss: 6.2289\n",
            "Epoch  6 | Loss: 6.1695\n",
            "Epoch  7 | Loss: 6.0858\n",
            "Epoch  8 | Loss: 5.9998\n",
            "Epoch  9 | Loss: 5.9300\n",
            "Epoch 10 | Loss: 5.8623\n",
            "Epoch 11 | Loss: 5.7979\n",
            "Epoch 12 | Loss: 5.7451\n",
            "Epoch 13 | Loss: 5.6796\n",
            "Epoch 14 | Loss: 5.6189\n",
            "Epoch 15 | Loss: 5.5692\n",
            "Epoch 16 | Loss: 5.5186\n",
            "Epoch 17 | Loss: 5.4794\n",
            "Epoch 18 | Loss: 5.4349\n",
            "Epoch 19 | Loss: 5.3976\n",
            "Epoch 20 | Loss: 5.3670\n",
            "Epoch 21 | Loss: 5.3219\n",
            "Epoch 22 | Loss: 5.2946\n",
            "Epoch 23 | Loss: 5.2617\n",
            "Epoch 24 | Loss: 5.2360\n",
            "Epoch 25 | Loss: 5.2141\n",
            "Epoch 26 | Loss: 5.1783\n",
            "Epoch 27 | Loss: 5.1506\n",
            "Epoch 28 | Loss: 5.1240\n",
            "Epoch 29 | Loss: 5.1140\n",
            "Epoch 30 | Loss: 5.0949\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.word2idx['<PAD>'])\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt, teacher_forcing_ratio=0.5)\n",
        "\n",
        "        loss = criterion(output[:, 1:].reshape(-1, output.size(-1)), tgt[:, 1:].flatten())\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Huấn luyện\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    print(f\"Epoch {epoch:2d} | Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnRidRm4CGTJ",
        "outputId": "864bdceb-dfc5-4c86-9d27-4c88720227a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCt4H84ECXS9",
        "outputId": "82ee60eb-ffbe-4897-9fc7-a92aa8a5f32d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq_vDP5K5NWB",
        "outputId": "9e159633-e835-40ae-9627-34a1f06c03c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đang đánh giá trên tập test...\n",
            "Đã sinh 2000 câu dự đoán.\n",
            "\n",
            "BLEU@1: 0.0000\n",
            "BLEU@2: 0.0000\n",
            "BLEU@3: 0.0000\n",
            "BLEU@4: 0.0000\n",
            "ROUGE-1: 0.0000\n",
            "ROUGE-2: 0.0000\n",
            "ROUGE-L: 0.0000\n",
            "METEOR: 0.0000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk import word_tokenize\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Hàm decode câu từ list indices → chuỗi text\n",
        "def decode_sentence(indices, vocab):\n",
        "    words = []\n",
        "    for idx in indices:\n",
        "        if idx == vocab.word2idx['<EOS>']:\n",
        "            break\n",
        "        if idx not in [vocab.word2idx['<PAD>'], vocab.word2idx['<SOS>'], vocab.word2idx['<UNK>']]:\n",
        "            word = vocab.idx2word.get(idx, '<UNK>')\n",
        "            words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Hàm đánh giá\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    hypotheses = []  # dự đoán\n",
        "    references = []  # ground truth\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            src = src.to(device)\n",
        "\n",
        "            # Encode source\n",
        "            hidden, cell = model.encoder(src)\n",
        "\n",
        "            batch_size = src.size(0)\n",
        "            max_len = 50  # giới hạn độ dài sinh\n",
        "\n",
        "            # Bắt đầu decoding với <SOS>\n",
        "            input_token = torch.full((batch_size, 1), tgt_vocab.word2idx['<SOS>'],\n",
        "                                   dtype=torch.long, device=device)\n",
        "            pred_sequences = []\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "                next_token = output.argmax(dim=-1)  # greedy\n",
        "                input_token = next_token\n",
        "                pred_sequences.append(next_token.squeeze(1).cpu())\n",
        "\n",
        "            # Ghép lại thành tensor (batch, seq_len)\n",
        "            pred_seq = torch.stack(pred_sequences, dim=1)\n",
        "\n",
        "            # Decode từng câu trong batch\n",
        "            for i in range(batch_size):\n",
        "                pred_str = decode_sentence(pred_seq[i], tgt_vocab)\n",
        "                ref_str  = decode_sentence(tgt[i].tolist(), tgt_vocab)\n",
        "\n",
        "                hypotheses.append(pred_str)\n",
        "                references.append(ref_str)\n",
        "\n",
        "    print(f\"Đã sinh {len(hypotheses)} câu dự đoán.\\n\")\n",
        "\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    bleu1_scores = [sentence_bleu([ref.split()], hyp.split(), weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
        "                    for hyp, ref in zip(hypotheses, references)]\n",
        "    bleu2_scores = [sentence_bleu([ref.split()], hyp.split(), weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
        "                    for hyp, ref in zip(hypotheses, references)]\n",
        "    bleu3_scores = [sentence_bleu([ref.split()], hyp.split(), weights=(1/3, 1/3, 1/3, 0), smoothing_function=smoothie)\n",
        "                    for hyp, ref in zip(hypotheses, references)]\n",
        "    bleu4_scores = [sentence_bleu([ref.split()], hyp.split(), weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
        "                    for hyp, ref in zip(hypotheses, references)]\n",
        "\n",
        "    print(f\"BLEU@1: {np.mean(bleu1_scores):.4f}\")\n",
        "    print(f\"BLEU@2: {np.mean(bleu2_scores):.4f}\")\n",
        "    print(f\"BLEU@3: {np.mean(bleu3_scores):.4f}\")\n",
        "    print(f\"BLEU@4: {np.mean(bleu4_scores):.4f}\")\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge1, rouge2, rougeL = [], [], []\n",
        "\n",
        "\n",
        "gdown.download_folder(id=folder_id, quiet=False, use_cookies=False)\n",
        "    for hyp, ref in zip(hypotheses, references):\n",
        "        scores = scorer.score(ref, hyp)\n",
        "        rouge1.append(scores['rouge1'].fmeasure)\n",
        "        rouge2.append(scores['rouge2'].fmeasure)\n",
        "        rougeL.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    print(f\"ROUGE-1: {np.mean(rouge1):.4f}\")\n",
        "    print(f\"ROUGE-2: {np.mean(rouge2):.4f}\")\n",
        "    print(f\"ROUGE-L: {np.mean(rougeL):.4f}\")\n",
        "\n",
        "    meteor_scores = []\n",
        "    for hyp, ref in zip(hypotheses, references):\n",
        "        # Tokenize trước khi tính METEOR\n",
        "        hyp_tokens = word_tokenize(hyp)\n",
        "        ref_tokens = word_tokenize(ref)\n",
        "        score = meteor_score([ref_tokens], hyp_tokens)  # list của reference\n",
        "        meteor_scores.append(score)\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"METEOR: {np.mean(meteor_scores):.4f}\")\n",
        "\n",
        "print(\"Đang đánh giá trên tập test...\")\n",
        "evaluate(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
